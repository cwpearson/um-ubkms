<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Microbenchmarking Unified Memory in CUDA 6.0</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="title" content="Microbenchmarking Unified Memory in CUDA 6.0"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2014-07-08 14:40:24 CDT"/>
<meta name="author" content="Sreepathi Pai"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style><link rel='stylesheet' type='text/css' href='style.css'>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Microbenchmarking Unified Memory in CUDA 6.0</h1>







<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Introduction</a></li>
<li><a href="#sec-2">2 Setup</a></li>
<li><a href="#sec-3">3 Microbenchmarks</a>
<ul>
<li><a href="#sec-3-1">3.1 automem</a></li>
<li><a href="#sec-3-2">3.2 nokernel</a></li>
<li><a href="#sec-3-3">3.3 readonly</a></li>
<li><a href="#sec-3-4">3.4 multivars</a></li>
<li><a href="#sec-3-5">3.5 privmem</a></li>
</ul>
</li>
<li><a href="#sec-4">4 Download</a></li>
<li><a href="#sec-5">5 Conclusions</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Introduction</h2>
<div class="outline-text-2" id="text-1">


<p>
  CUDA 6.0 introduced <a href="http://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/"><i>Unified Memory</i></a>, a new feature that makes it easier to write CUDA programs by automating data transfers between the CPU and GPU.
  A key problem with any automated data transfer solution is that it can introduce <i>redundant</i> transfers. 
  We studied this problem in our <a href="http://dl.acm.org/citation.cfm?id=2370816.2370824&amp;coll=DL&amp;dl=GUIDE&amp;CFID=374283105&amp;CFTOKEN=50580172">PACT 2012 paper</a> and concluded that redundant transfers can only be minimized by maintaining <i>full</i> runtime coherence status on both the CPU and GPU.
</p>
<p>
  In this document, I evaluate CUDA 6.0's Unified Memory (UM) mechanism on a Kepler K20Xm using a number of microbenchmarks derived from our previous work. 
  These microbenchmarks are described in the sections below and <a href="um-ubmks.tar.bz2">the source code</a> is also available for download.
  Using these microbenchmarks, I find that UM can introduce redundant transfers in common usage patterns.
</p>
<p>
  The microbenchmarks show that:
</p><ul>
<li>UM assumes that the GPU always has non-stale (i.e. "freshest") data. This leads to redundant <i>non-stale</i> to <i>non-stale</i> transfers from the GPU to the CPU.
</li>
<li>UM does not check if the GPU actually needs the data being transferred from the CPU. This leads to potentially redundant eager transfers.
</li>
</ul>


<p>
  In the style of Table 1 in our paper, UM can be summarized as:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" /><col class="left" /><col class="left" /><col class="left" />
</colgroup>
<thead>
<tr><th scope="col" class="left">Scheme</th><th scope="col" class="left">Only Non-stale to  Stale</th><th scope="col" class="left"></th><th scope="col" class="left">Prevents Eager</th><th scope="col" class="left"></th></tr>
</thead>
<tbody>
<tr><td class="left"></td><td class="left">CPU-to-GPU</td><td class="left">GPU-to-CPU</td><td class="left">CPU-to-GPU</td><td class="left">GPU-to-CPU</td></tr>
</tbody>
<tbody>
<tr><td class="left">CUDA 6.0 UM</td><td class="left">Y</td><td class="left"><a href="#sec-3-2">N</a></td><td class="left"><a href="#sec-3-4">N</a></td><td class="left">Y</td></tr>
</tbody>
</table>



</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Setup</h2>
<div class="outline-text-2" id="text-2">


<p>
  The hardware and software setup used herein is:
</p>


<pre class="example">
GPU: Tesla K20Xm (3.5)
Managed memory supported: 1
Driver: 6000
Runtime: 6000
</pre>


<p>  
  We use the nvprof command line profiler to obtain the following statistics:
</p><ul>
<li>CPU Page Faults
</li>
<li>Bytes transferred from GPU to CPU (DtoH)
</li>
<li>Bytes transferred from CPU to GPU (HtoD)
</li>
</ul>


</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Microbenchmarks</h2>
<div class="outline-text-2" id="text-3">


</div>

<div id="outline-container-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> automem</h3>
<div class="outline-text-3" id="text-3-1">


<p>   
   The <a href="automem.cu">automem</a> microbenchmark verifies Unified Memory exists and is working correctly. 
   As the graph below shows, it allocates a Unified Memory variable <i>y</i>, initializes it on the CPU, then invokes a kernel that doubles the value of the variable, and finally the CPU prints out the doubled value.
   All the square boxes in the graph are CPU actions, while ellipses are GPU actions. 
   The circular SYNC corresponds to a <code>cudaDeviceSynchronize()</code> call.
   The labels on the edges are explained below.
</p>


<p>
<img src="automem.png"  alt="automem.png" />
</p>
<p>
The profiler output for automem is:
</p>



<table>
<tr> <th align=left>Unified Memory CPU page faults</th> <td align=right>2</td> </tr>
<tr> <th align=left>Unified Memory Memcpy DtoH</th> <td align=right>8192</td> </tr>
<tr> <th align=left>Unified Memory Memcpy HtoD</th> <td align=right>4096</td> </tr>
</table>

<p>
   Each page fault corresponds exactly to a DtoH transfer and are marked in the graph.
   I assume HtoD transfers only occur on a kernel call.
   In the graph, the green arrows indicate <i>required</i> transfers while the red dashed arrows indicate <i>redundant</i> transfers.
</p>
<p>   
   For automem, it is clear that a page has to be transferred from the CPU to the GPU ("HtoD") after initialization since the CPU has the non-stale copy.
   It is also clear that a page must to be transferred back from the GPU to the CPU ("DtoH") since the GPU modifies the shared variable.
   This happens when the CPU attempts to print the value of <i>y</i> after the kernel.
   However, the DtoH that occurs when <i>y</i> is being initialized is not necessary &ndash; neither the GPU nor the CPU have the non-stale copy immediately after the allocation of a Unified Memory variable.
   This page fault during initialization can only happen if UM assumes that the GPU has a non-stale copy after allocation while the CPU has a stale copy.
   The nokernel microbenchmark below shows that this is indeed the case.
</p>
</div>

</div>

<div id="outline-container-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> nokernel</h3>
<div class="outline-text-3" id="text-3-2">


<p>
   The <a href="nokernel.cu">nokernel</a> microbenchmark allocates a Unified Memory variable and initializes it on the CPU. There are no kernel calls.
</p>


<p>
<img src="nokernel.png"  alt="nokernel.png" />
</p>
<p>
The profiler output for nokernel is:
</p>



<table>
<tr> <th align=left>Unified Memory CPU page faults</th> <td align=right>1</td> </tr>
<tr> <th align=left>Unified Memory Memcpy DtoH</th> <td align=right>4096</td> </tr>
</table>

<p>
   The profiler log shows that nokernel exhibits a page fault <i>and</i> a transfer from the GPU to the CPU.
   The initial coherence status assigned to the devices is therefore not the same, but the GPU is always assumed to contain the non-stale data initially.
   Thus, initializing data on the CPU, a very common pattern, will incur redundant data DtoH transfers under UM.
   Further, as the the next microbenchmark readonly shows, UM <i>always</i> assumes the GPU has a non-stale copy after a kernel call too.
</p>
</div>

</div>

<div id="outline-container-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> readonly</h3>
<div class="outline-text-3" id="text-3-3">


<p>
   The <a href="readonly.cu">readonly</a> microbenchmark contains a kernel that writes to a Unified Memory variable <i>conditionally</i>. In the standard run, the kernel <i>does not</i> write to the Unified Memory variable.
</p>


<p>
<img src="readonly.png"  alt="readonly.png" />
</p>
<p>
The profiler output for readonly is:
</p>



<table>
<tr> <th align=left>Unified Memory CPU page faults</th> <td align=right>2</td> </tr>
<tr> <th align=left>Unified Memory Memcpy DtoH</th> <td align=right>8192</td> </tr>
<tr> <th align=left>Unified Memory Memcpy HtoD</th> <td align=right>4096</td> </tr>
</table>

<p>
Since the kernel did not change <i>y</i>, there is no need for a DtoH transfer when the CPU reads <i>y</i>.
But as the profiler shows, readonly incurs page faults both during initialization as well as during the read of <i>y</i>. 
So it seems that in UM, a kernel is always assumed to write to Unified Memory variables.
Therefore, Unified Memory variables that are only read by GPU will incur redundant transfers when accessed by the CPU after a kernel call. 
Further, as the multivars microbenchmark next shows, there seems to be no hardware ability to detect which variables a GPU kernel reads and/or writes.
</p>
</div>

</div>

<div id="outline-container-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> multivars</h3>
<div class="outline-text-3" id="text-3-4">


<p>
   The <a href="multivars.cu">multivars</a> microbenchmark initializes two Unified Memory variables, <i>y</i> and <i>z</i>, where <i>z</i> is not used by the kernel.
   The variables <i>y</i> and <i>z</i> are sized to span 1 and 160 CPU pages respectively.
</p>


<p>
<img src="multivars.png"  alt="multivars.png" />
</p>
<p>
The profiler log for multivars is:
</p>



<table>
<tr> <th align=left>Unified Memory CPU page faults</th> <td align=right>321</td> </tr>
<tr> <th align=left>Unified Memory Memcpy DtoH</th> <td align=right>1314816</td> </tr>
<tr> <th align=left>Unified Memory Memcpy HtoD</th> <td align=right>655360</td> </tr>
</table>

<p>
The log demonstrates that UM transfers all CPU-modified shared data, regardless of whether a GPU kernel actually reads that data.
Here, the HtoD numbers reflect the redundant eager transfer of <i>z</i>'s 160 pages. 
The DtoH numbers reflect 160 pages of <i>z</i> transferred during initialization and after the kernel call, 160 pages of <i>z</i> + 1 page of <i>y</i>.
</p>
<p>
Since UM supports recursive data structures (like trees, linked lists, etc.), it cannot examine kernel arguments to limit which pages must be transferred.
</p>

</div>

</div>

<div id="outline-container-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> privmem</h3>
<div class="outline-text-3" id="text-3-5">


<p>
   The <a href="privmem.cu">privmem</a> microbenchmark exercises an important use-case (GPU private memory) where GPU kernels share some data amongst themselves but the CPU never reads or writes to it.
   Ideally, we should never see any Unified Memory transfers of such data.
</p>


<p>
<img src="privmem.png"  alt="privmem.png" />
</p>
<p>
As expected, the profiler does not log any Unified Memory transfers, so UM does the right thing.
</p>



<table>
</table>

</div>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> Download</h2>
<div class="outline-text-2" id="text-4">


<p>
The source code and supporting scripts for the <a href="um-ubmks.tar.bz2">UM microbenchmarks</a> is available.
</p>
</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Conclusions</h2>
<div class="outline-text-2" id="text-5">


<p>
CUDA 6.0's UM evaluated on a Kepler K20Xm:
</p><ul>
<li>assumes that the GPU always contains the most up-to-date (non-stale) data
</li>
<li>transfers all modified data from the CPU to the GPU regardless of whether it will be read, causing redundant eager transfers
</li>
<li>transfers all data from the GPU to the CPU even if the GPU did not modify the data, causing redundant non-stale to non-stale transfers
</li>
</ul>


<p>
Thus, it exhibits redundant transfers.
</p>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2014-07-08 14:40:24 CDT</p>
<p class="author">Author: Sreepathi Pai</p>
<p class="creator">Org version 7.8.02 with Emacs version 23</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
